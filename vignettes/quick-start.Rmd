---
title: "Quick Start"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Setup 

```{r setup}
# assumes you have client JSON saved to environment argument GAR_CLIENT_JSON
app_project <- googleAuthR::gar_set_client(
  scopes = c("https://www.googleapis.com/auth/cloud-platform"))

library(googleAuthR)
library(googleCloudAutoMLTablesR)
options(googleAuthR.verbose = 0)

gar_auth("gcat.oauth")
```

Test authentication and check in correct project: 

```{r}
# assumes you have projectId string set to environment 
# argument GCAT_PROJECT_ID
projectId <- Sys.getenv("GCAT_PROJECT_ID")

## Get list of locations 
gcat_locations <- gcat_list_locations(projectId = projectId)
gcat_locations$locations
```

## Set location 

```{r}
## manually set location path TODO - @justinjm - update function to handle 
## location setting 
location <- "us-central1"
gcat_location_path(projectId = projectId, 
                   location = location)
```

## Get list of datasets 

```{r}
datasets_list <- gcat_list_datasets(projectId = projectId,
                                    location = location)
datasets_list
```

# Importing data into AutoML Tables 

## Options and Workflow

There are 2 options for loading data into AutoML Tables

1. Google Cloud Storage 
2. BigQuery

At a high-level, the workflow is:

1. Locate your data in GCP (csv file in GCS bucket or csv file loaded into a BQ table)
1. Create AutoML Tables dataset
2. Import data from GCS or BQ into AutoML Tables dataset 

## From Google Cloud Storage

To load data into AutoML Tables from Google Cloud storage, first do the following:

1. Setup a GCS bucket that is [regional](https://cloud.google.com/automl-tables/docs/prepare#bucket_requirements) and in `us-central1` region
2. Upload your data file manually or via `googleCloudStorageR`

Now we are ready to create an AutoML tables dataset.

### Create AutoML Tables dataset 

For now, one has to manually create dataset via [GCP console](https://console.cloud.google.com/automl-tables/datasets/) 

```{r}
# TODO @justinjm - fix function, issue #1 
# https://github.com/justinjm/googleCloudAutoMLTablesR/issues/1  
# gcat_dataset <- gcat_create_dataset(projectId = projectId,
#                                     location = location,
#                                     displayName = "test_01")
```

### Import data 

Execute import from Google Cloud storage: 

```{r}
# gcat_import_data(
#   projectId = projectId,
#   location = location,
#   dataset_display_name = "test_01",
#   input_source = "gcs",
#   input_url = "gs://gcatr-dev/bank_marketing.csv"
#   )
```

## From BigQuery 

To load data into AutoML Tables from BigQuery, first do the following:

1. Setup a BQ dataset 
2. Upload your data file manually or via `bigQueryR` or `bigrquery`

### Create AutoML Tables dataset 

For now, one has to manually create dataset via [GCP console](https://console.cloud.google.com/automl-tables/datasets/) 

```{r}
# TODO @justinjm - fix function, issue #1 
# https://github.com/justinjm/googleCloudAutoMLTablesR/issues/1  
# gcat_dataset <- gcat_create_dataset(projectId = projectId,
#                                     location = location,
#                                     displayName = "test_01_bq")
```

### Import data 

Execute import from BigQuery: 

```{r}
# gcat_import_data(
#   projectId = projectId,
#   location = location,
#   dataset_display_name = "test_01_bq",
#   input_source = "bq",
#   input_url = "bq://gc-automl-tables-r.gcatr_dev.bank_marketing"
#   )
```


## View dataset 

Once the import - from GCS or BQ - is completed, you'll recieve an email notification. Locate the `datasetId` in the GCP console or via `gcat_list_datasets` function. (e.g. - will be in the form `TBL123456789`) Once the data import is completed, we can then sanity check the results.

First, let's set our `dataset_display_name` and `datasetId` in advance for cleaner code later:

```{r}
dataset_display_name <- "test_01_bq"
datasetId <- "TBL4800700863335104512"
```

Now we're ready to view the results and get the `tableSpecId` we need for later functions:

```{r}
dataset <- gcat_get_dataset(projectId = projectId,
                            location = location,
                            datasetId = datasetId)
```

# Updating dataset in AutoML Tables

AutoML Tables automatically detects your data column type. Depending on the type of your label column, AutoML Tables chooses to run a classification or regression model. If your label column contains only numerical values, but they represent categories, change your label column type to categorical by updating your schema.

Lets first view the table schema and the columns

## List table specs 

```{r}
table_specs <- gcat_list_table_specs(
  projectId = projectId,
  location = location,
  dataset_display_name = dataset_display_name 
  )
table_specs
```

## List column specs 

Use Primary Table Spec ID (tableSpecId) to get the column specs:  

```{r}
table_colspecs_list <- gcat_list_column_specs(
  projectId = projectId,
  location = location,
  datasetId = datasetId,
  tableSpecId = "7338035050660757504"
)
table_colspecs_list
```

## Visualize column specs 

## Set label or target column 

Set V16 or outcome [source](https://datahub.io/machine-learning/bank-marketing) as label

```{r}
# library(jsonlite)
dataset2 <- gcat_set_label(
  projectId = projectId,
  location = location,
  datasetId = datasetId,
  tableSpecId = "7338035050660757504",
  label_column_name = "V16" 
  )
dataset2
```



```{r VIEW-DEBUG}
# view debugging info
readRDS("request_debug.rds")
```


### Create model 

## Modeling 

### Train Model

### Make batch prediction 

### Evaluate predictions 
